# Benchmark config for TrainPipelineSparseDistBWDOpt with Shampoo optimizer
# Based on sparse_data_dist_bwd_opt, with Shampoo for dense parameters.
# Shampoo's expensive second-order optimizer step should benefit more from
# being overlapped with backward all-to-all communication.
RunOptions:
  world_size: 2
  num_batches: 10
  num_benchmarks: 1
  num_profiles: 1
  memory_snapshot: True
  profile_dir: "."
  name: "sparse_data_dist_shampoo"
  loglevel: "info"
PipelineConfig:
  pipeline: "sparse" # "sparse-bwd-opt"
  kwargs:
    site_fqn: "sparse.weighted_ebc"
    sharding_type: "table_wise"
ModelInputConfig:
  num_float_features: 100
  feature_pooling_avg: 30
ModelSelectionConfig:
  model_name: "test_sparse_nn"
  model_config:
    num_float_features: 100
    submodule_kwargs:
      dense_arch_out_size: 1024
      over_arch_out_size: 4096
      over_arch_hidden_layers: 5
      dense_arch_hidden_sizes: [1024, 1024, 1024]
EmbeddingTablesConfig:
  num_unweighted_features: 90
  num_weighted_features: 80
  embedding_feature_dim: 256
  additional_tables:
    - - name: FP16_table
        embedding_dim: 512
        num_embeddings: 100_000
        feature_names: ["additional_0_0"]
        data_type: FP16
      - name: large_table
        embedding_dim: 2048
        num_embeddings: 1_000_000
        feature_names: ["additional_0_1"]
    - []
    - - name: skipped_table
        embedding_dim: 128
        num_embeddings: 100_000
        feature_names: ["additional_2_1"]
PlannerConfig:
  sharding_type: table_wise
  pooling_factors: [30.0]  # Must match ModelInputConfig.feature_pooling_avg
  hardware:  # A100
    hbm_cap: 85899345920  # 80GB
  additional_constraints:
    large_table:
      sharding_types: [column_wise]
ShardingConfig:
  dense_optimizer: "Shampoo"
  dense_lr: 0.01
  fused_params:
    optimizer: EXACT_ROWWISE_ADAGRAD
    learning_rate: 0.01

# TorchRec Cloud Deployment Guide

This guide provides comprehensive examples for deploying TorchRec distributed training
on major cloud providers: **AWS**, **Microsoft Azure**, and **Google Cloud Platform (GCP)**.

## Cloud Deployment Architecture Overview

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TorchRec Cloud Deployment Architecture                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                         â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                              â”‚  torchrun / Kubeflow    â”‚                                â”‚
â”‚                              â”‚   Training Operator     â”‚                                â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                          â”‚                                              â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚            â”‚                             â”‚                             â”‚                â”‚
â”‚            â–¼                             â–¼                             â–¼                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚      AWS        â”‚           â”‚     Azure       â”‚           â”‚      GCP        â”‚       â”‚
â”‚   â”‚   EKS / EC2     â”‚           â”‚   AKS / VMs     â”‚           â”‚   GKE / VMs     â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚            â”‚                             â”‚                             â”‚                â”‚
â”‚            â–¼                             â–¼                             â–¼                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ p4d.24xlarge    â”‚           â”‚ ND96asr_v4      â”‚           â”‚ a2-highgpu-8g   â”‚       â”‚
â”‚   â”‚ 8x A100 (40GB)  â”‚           â”‚ 8x A100 (40GB)  â”‚           â”‚ 8x A100 (40GB)  â”‚       â”‚
â”‚   â”‚ 96 vCPUs        â”‚           â”‚ 96 vCPUs        â”‚           â”‚ 96 vCPUs        â”‚       â”‚
â”‚   â”‚ 1.1TB RAM       â”‚           â”‚ 900GB RAM       â”‚           â”‚ 1.3TB RAM       â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚            â”‚                             â”‚                             â”‚                â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                          â”‚                                              â”‚
â”‚                                          â–¼                                              â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                              â”‚   TorchRec Training     â”‚                                â”‚
â”‚                              â”‚  DistributedModelParallelâ”‚                               â”‚
â”‚                              â”‚  TrainPipelineSparseDist â”‚                               â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Cloud Provider Comparison

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GPU Instance Comparison for TorchRec Training                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                         â”‚
â”‚  Provider    Instance Type        GPUs           GPU Memory    Network        Cost/hr   â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                                         â”‚
â”‚  AWS         p4d.24xlarge        8x A100        40GB each     400 Gbps EFA   ~$32      â”‚
â”‚              p4de.24xlarge       8x A100        80GB each     400 Gbps EFA   ~$40      â”‚
â”‚              p5.48xlarge         8x H100        80GB each     3200 Gbps EFA  ~$98      â”‚
â”‚                                                                                         â”‚
â”‚  Azure       ND96asr_v4          8x A100        40GB each     200 Gbps IB    ~$27      â”‚
â”‚              ND96amsr_A100_v4    8x A100        80GB each     200 Gbps IB    ~$33      â”‚
â”‚              ND96isr_H100_v5     8x H100        80GB each     400 Gbps IB    ~$85      â”‚
â”‚                                                                                         â”‚
â”‚  GCP         a2-highgpu-8g       8x A100        40GB each     100 Gbps       ~$29      â”‚
â”‚              a2-ultragpu-8g      8x A100        80GB each     100 Gbps       ~$40      â”‚
â”‚              a3-highgpu-8g       8x H100        80GB each     200 Gbps       ~$80      â”‚
â”‚                                                                                         â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                                         â”‚
â”‚  Recommendation for TorchRec:                                                           â”‚
â”‚  â€¢ Training: p4d.24xlarge (AWS), ND96asr_v4 (Azure), a2-highgpu-8g (GCP)               â”‚
â”‚  â€¢ Large embeddings (>40GB): Use 80GB variants                                          â”‚
â”‚  â€¢ Multi-node: Prioritize high network bandwidth (EFA/InfiniBand)                       â”‚
â”‚                                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Quickstart: Run TorchRec in 5 Minutes

**New to TorchRec?** Start with our [Quickstart Guide](quickstart/README.md) for a ready-to-run
example that works on any cloud provider!

```bash
# Build and run locally (single GPU)
cd quickstart && docker build -t torchrec-quickstart . && docker run --gpus all torchrec-quickstart train_torchrec_quickstart.py

# Or deploy to Kubernetes (multi-GPU, multi-node)
kubectl apply -f quickstart/kubernetes_job.yaml
```

The quickstart includes:
- âœ… Self-contained training script with synthetic data (no downloads needed!)
- âœ… Dockerfile for containerized deployment
- âœ… Kubernetes manifest for cloud deployment
- âœ… Works on AWS, Azure, and GCP

## Directory Structure

```text
cloud_deployment/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ quickstart/                  # â­ Start here!
â”‚   â”œâ”€â”€ README.md               # Quickstart guide
â”‚   â”œâ”€â”€ train_torchrec_quickstart.py  # Self-contained training script
â”‚   â”œâ”€â”€ Dockerfile              # Container for cloud deployment
â”‚   â”œâ”€â”€ kubernetes_job.yaml     # Kubernetes job manifest
â”‚   â””â”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ aws/
â”‚   â”œâ”€â”€ README.md               # AWS-specific deployment guide
â”‚   â”œâ”€â”€ eks_cluster.yaml        # EKS cluster configuration
â”‚   â”œâ”€â”€ cloud_component.py     # TorchX component for AWS (legacy)
â”‚   â””â”€â”€ train_dlrm_aws.sh       # AWS training script
â”œâ”€â”€ azure/
â”‚   â”œâ”€â”€ README.md               # Azure-specific deployment guide
â”‚   â”œâ”€â”€ aks_cluster.yaml        # AKS cluster configuration
â”‚   â”œâ”€â”€ cloud_component.py     # TorchX component for Azure (legacy)
â”‚   â””â”€â”€ train_dlrm_azure.sh     # Azure training script
â””â”€â”€ gcp/
    â”œâ”€â”€ README.md               # GCP-specific deployment guide
    â”œâ”€â”€ gke_cluster.yaml        # GKE cluster configuration
    â”œâ”€â”€ cloud_component.py     # TorchX component for GCP (legacy)
    â””â”€â”€ train_dlrm_gcp.sh       # GCP training script
```

## Quick Start

### Prerequisites

1. **PyTorch with torchrun** (included with PyTorch >= 1.9):
   ```bash
   pip install torch torchrec fbgemm-gpu
   ```

2. **Install Cloud CLI Tools**:
   ```bash
   # AWS
   pip install awscli boto3

   # Azure
   pip install azure-cli

   # GCP
   pip install google-cloud-sdk
   ```

3. **Configure Cloud Credentials**:
   ```bash
   # AWS
   aws configure

   # Azure
   az login

   # GCP
   gcloud auth login
   ```

### Deploy TorchRec Training

Choose your cloud provider:

| Provider | Guide | Quick Command |
|----------|-------|---------------|
| AWS | [aws/README.md](aws/README.md) | `torchrun --nnodes=2 --nproc_per_node=8 ...` |
| Azure | [azure/README.md](azure/README.md) | `torchrun --nnodes=2 --nproc_per_node=8 ...` |
| GCP | [gcp/README.md](gcp/README.md) | `torchrun --nnodes=2 --nproc_per_node=8 ...` |

## Training Flow on Cloud

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TorchRec Cloud Training Workflow                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                         â”‚
â”‚   1. SETUP PHASE                                                                        â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                                        â”‚
â”‚                                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚   â”‚  Provision  â”‚â”€â”€â”€â–ºâ”‚   Upload    â”‚â”€â”€â”€â–ºâ”‚  Configure  â”‚â”€â”€â”€â–ºâ”‚   Deploy    â”‚             â”‚
â”‚   â”‚  GPU Clusterâ”‚    â”‚   Dataset   â”‚    â”‚  Kubernetes â”‚    â”‚    Job      â”‚             â”‚
â”‚   â”‚  (EKS/AKS/  â”‚    â”‚  (S3/Blob/  â”‚    â”‚  + torchrun â”‚    â”‚             â”‚             â”‚
â”‚   â”‚   GKE)      â”‚    â”‚   GCS)      â”‚    â”‚             â”‚    â”‚             â”‚             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                                         â”‚
â”‚   2. TRAINING PHASE                                                                     â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                                     â”‚
â”‚                                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                         Kubernetes Cluster                                       â”‚  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚   â”‚  â”‚                    TorchRec Distributed Training                          â”‚  â”‚  â”‚
â”‚   â”‚  â”‚                                                                           â”‚  â”‚  â”‚
â”‚   â”‚  â”‚   Node 0 (Rank 0-7)           Node 1 (Rank 8-15)                          â”‚  â”‚  â”‚
â”‚   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚  â”‚  â”‚
â”‚   â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”   â”‚        â”‚ â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”   â”‚                          â”‚  â”‚  â”‚
â”‚   â”‚  â”‚  â”‚ â”‚GPU0â”‚ â”‚GPU1â”‚...â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚ â”‚GPU0â”‚ â”‚GPU1â”‚...â”‚   NCCL All-to-All       â”‚  â”‚  â”‚
â”‚   â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜   â”‚  High  â”‚ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜   â”‚   Communication          â”‚  â”‚  â”‚
â”‚   â”‚  â”‚  â”‚ Embedding Shardsâ”‚  BW    â”‚ Embedding Shardsâ”‚                          â”‚  â”‚  â”‚
â”‚   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ NVLinkâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚  â”‚  â”‚
â”‚   â”‚  â”‚                       + EFA/IB                                            â”‚  â”‚  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                                         â”‚
â”‚   3. MONITORING & CHECKPOINTING                                                         â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                        â”‚
â”‚                                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚   â”‚  TensorBoardâ”‚    â”‚ Checkpoint  â”‚    â”‚   Metrics   â”‚                                â”‚
â”‚   â”‚  (Training  â”‚    â”‚  to Cloud   â”‚    â”‚  Dashboard  â”‚                                â”‚
â”‚   â”‚   Curves)   â”‚    â”‚   Storage   â”‚    â”‚ (CloudWatch/â”‚                                â”‚
â”‚   â”‚             â”‚    â”‚             â”‚    â”‚  Stackdriver)â”‚                               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Best Practices for Cloud Deployment

### 1. Data Storage Strategy

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Data Storage Recommendations                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                         â”‚
â”‚  Cloud      Raw Data          Preprocessed Data        Checkpoints                      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•       â”‚
â”‚                                                                                         â”‚
â”‚  AWS        S3 Standard       S3 + FSx for Lustre     S3 Standard                      â”‚
â”‚             (cold storage)    (high-throughput I/O)   (durable)                         â”‚
â”‚                                                                                         â”‚
â”‚  Azure      Blob Storage      Azure NetApp Files      Blob Storage                     â”‚
â”‚             (cold tier)       (NFS mount)             (hot tier)                        â”‚
â”‚                                                                                         â”‚
â”‚  GCP        Cloud Storage     Filestore               Cloud Storage                    â”‚
â”‚             (standard)        (high-scale tier)       (standard)                        â”‚
â”‚                                                                                         â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•       â”‚
â”‚                                                                                         â”‚
â”‚  ðŸ’¡ TIP: For Criteo 1TB dataset, use parallel file systems (FSx/NetApp/Filestore)      â”‚
â”‚          to achieve >10 GB/s read throughput needed for large batch training           â”‚
â”‚                                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Network Configuration

- **Enable high-bandwidth networking**: EFA (AWS), InfiniBand (Azure), GPUDirect (GCP)
- **Place nodes in same availability zone** for lowest latency
- **Use placement groups** (AWS) or proximity placement groups (Azure) for multi-node

### 3. Cost Optimization

- Use **Spot/Preemptible instances** for fault-tolerant training with checkpointing
- **Right-size instances**: Start with smaller GPU counts, scale up as needed
- **Enable auto-scaling** for inference workloads

## Troubleshooting

| Issue | Solution |
|-------|----------|
| NCCL timeout | Increase `NCCL_IB_TIMEOUT`, check security groups |
| OOM on GPU | Reduce batch size, enable gradient checkpointing |
| Slow data loading | Use cloud-native parallel file systems |
| Job preemption | Enable checkpointing every N iterations |

## Related Examples

- [golden_training/](../golden_training/) - Reference DLRM training implementation
- [ray/](../ray/) - Ray cluster integration
- [nvt_dataloader/](../nvt_dataloader/) - NVTabular for GPU data loading

## References

- [torchrun Documentation](https://pytorch.org/docs/stable/elastic/run.html)
- [Kubeflow Training Operator](https://www.kubeflow.org/docs/components/trainer/overview/)
- [TorchX Documentation](https://pytorch.org/torchx/) (legacy)
- [AWS Deep Learning Containers](https://github.com/aws/deep-learning-containers)
- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/)
- [Google Cloud AI Platform](https://cloud.google.com/ai-platform)
